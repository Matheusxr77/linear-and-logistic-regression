import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.metrics import mean_squared_error, r2_score
from src.models import train_linear_regression
from src.plotting import (
    plot_regression_scatter,
    plot_predicted_vs_actual,
    plot_regression_confidence_interval,
    plot_regression_confusion_matrix
)

st.set_page_config(page_title="Regress√£o Linear", layout="wide")

st.title("Parte 1: An√°lise de Regress√£o Linear")
st.markdown("""
Nesta se√ß√£o, utilizamos a Regress√£o Linear M√∫ltipla para modelar a rela√ß√£o entre uma vari√°vel de desempenho (dependente) e uma ou mais vari√°veis estat√≠sticas (independentes).

**Equa√ß√£o da Regress√£o Linear:**
""")

st.latex(r"y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + \varepsilon")

st.markdown("""
**Hip√≥teses que podemos testar:**
- Um determinado Jogador far√° Y pontos?
- Um determinado Jogador far√° Y rebotes?
- Um determinado Jogador far√° Y assist√™ncias?
- O time far√° X Pontos no jogo?
- O time far√° X Rebotes no jogo?
- O time far√° X Assist√™ncias no jogo?

**Instru√ß√µes:**
1.  **Escolha o(s) Jogador(es):** Selecione um ou mais jogadores para an√°lise.
2.  **Escolha a Vari√°vel Dependente (Y):** Selecione a estat√≠stica que voc√™ deseja prever.
3.  **Escolha as Vari√°veis Independentes (X):** Selecione uma ou mais estat√≠sticas que voc√™ acredita que influenciam a vari√°vel dependente.
4.  Clique em **'Executar An√°lise'** para treinar o modelo e visualizar os resultados.
""")

# Verifica se os dados foram carregados e est√£o no estado da sess√£o
if 'player_data' not in st.session_state or st.session_state['player_data'].empty:
    st.error("Os dados dos jogadores n√£o foram carregados. Por favor, volte para a p√°gina principal (app.py) para iniciar o carregamento.")
else:
    player_df = st.session_state['player_data']

    # --- SELE√á√ÉO DE JOGADORES ---
    st.subheader("üèÄ Sele√ß√£o de Jogadores")
    
    # Identifica a coluna de nome do jogador
    name_column = None
    for col in ['PLAYER_NAME', 'Player_Name', 'PLAYER', 'Player']:
        if col in player_df.columns:
            name_column = col
            break
    
    if not name_column:
        st.error("N√£o foi poss√≠vel identificar a coluna com os nomes dos jogadores nos dados carregados.")
        st.info(f"Colunas dispon√≠veis: {', '.join(player_df.columns.tolist())}")
        st.stop()
    
    # Obt√©m lista √∫nica de jogadores
    available_players = sorted(player_df[name_column].unique().tolist())
    
    col1, col2 = st.columns([3, 1])
    with col1:
        selected_players = st.multiselect(
            "Selecione um ou mais jogadores para an√°lise:",
            options=available_players,
            default=[available_players[0]] if available_players else [],
            help="Voc√™ pode selecionar m√∫ltiplos jogadores para an√°lise comparativa"
        )
    
    with col2:
        st.metric("Jogadores dispon√≠veis", len(available_players))
        st.metric("Jogadores selecionados", len(selected_players))
    
    if not selected_players:
        st.warning("‚ö†Ô∏è Por favor, selecione pelo menos um jogador para continuar com a an√°lise.")
        st.stop()
    
    # Filtra dados pelos jogadores selecionados
    df = player_df[player_df[name_column].isin(selected_players)].copy()
    
    st.success(f"‚úì {len(selected_players)} jogador(es) selecionado(s): {', '.join(selected_players)}")
    
    # Mostra estat√≠sticas resumidas dos jogadores selecionados
    with st.expander("üìä Estat√≠sticas dos Jogadores Selecionados"):
        stats_cols = ['PTS', 'REB', 'AST', 'FG_PCT', 'FG3_PCT', 'FT_PCT', 'MIN']
        available_stats = [col for col in stats_cols if col in df.columns]
        
        if available_stats:
            summary_stats = df.groupby(name_column)[available_stats].agg(['mean', 'std', 'min', 'max'])
            st.dataframe(summary_stats.round(2), use_container_width=True)
        else:
            st.warning("Estat√≠sticas b√°sicas n√£o encontradas nos dados.")

    # Lista detalhada de jogadores (similar √† regress√£o log√≠stica)
    with st.expander("üìã Lista Detalhada de Jogadores Selecionados"):
        for player in selected_players:
            player_data = df[df[name_column] == player]
            
            col1, col2, col3 = st.columns([2, 1, 1])
            
            with col1:
                st.markdown(f"### {player}")
            
            with col2:
                st.metric("Jogos", len(player_data))
            
            with col3:
                if 'MIN' in player_data.columns:
                    avg_min = player_data['MIN'].mean()
                    st.metric("Min/Jogo", f"{avg_min:.1f}")
            
            # M√©tricas principais
            metrics_row = st.columns(5)
            
            metric_configs = [
                ('PTS', 'Pontos', 'üèÄ'),
                ('REB', 'Rebotes', 'üîÑ'),
                ('AST', 'Assist√™ncias', 'üéØ'),
                ('FG_PCT', 'FG%', 'üìä'),
                ('FG3_PCT', '3P%', 'üé™')
            ]
            
            for idx, (col_name, label, icon) in enumerate(metric_configs):
                if col_name in player_data.columns:
                    avg_value = player_data[col_name].mean()
                    if 'PCT' in col_name:
                        metrics_row[idx].metric(f"{icon} {label}", f"{avg_value:.1%}")
                    else:
                        metrics_row[idx].metric(f"{icon} {label}", f"{avg_value:.1f}")
            
            st.divider()

    # Define as colunas num√©ricas que podem ser usadas como vari√°veis
    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
    # Remove colunas de ID que n√£o s√£o √∫teis para an√°lise
    exclude_cols = ['PLAYER_ID', 'TEAM_ID', 'GAME_ID', 'WIN']
    numeric_cols = [col for col in numeric_cols if col not in exclude_cols]

    # --- Abas de Navega√ß√£o ---
    tabs = st.tabs(["üìä An√°lise Principal", "üìà Explora√ß√£o de Dados", "üéÅ An√°lise de Res√≠duos"])

    # ============================================================================
    # ABA 1: AN√ÅLISE PRINCIPAL
    # ============================================================================
    with tabs[0]:
        st.subheader("Modelo de Regress√£o Linear")

        col1, col2 = st.columns(2)
        with col1:
            dependent_var = st.selectbox(
                "1. Escolha a Vari√°vel Dependente (Y) para prever:",
                options=numeric_cols,
                index=numeric_cols.index('PTS') if 'PTS' in numeric_cols else 0,
                help="Esta √© a vari√°vel que o modelo tentar√° prever."
            )

        available_independent_vars = [v for v in numeric_cols if v != dependent_var]

        with col2:
            independent_vars = st.multiselect(
                "2. Escolha as Vari√°veis Independentes (X):",
                options=available_independent_vars,
                default=[available_independent_vars[0]] if available_independent_vars else [],
                help="Estas s√£o as vari√°veis que o modelo usar√° para fazer a previs√£o."
            )

        if st.button("Executar An√°lise de Regress√£o Linear", type="primary"):
            if not independent_vars:
                st.warning("Por favor, selecione pelo menos uma vari√°vel independente.")
            else:
                # Verifica quantidade de dados v√°lidos antes de treinar
                df_valid = df.dropna(subset=[dependent_var] + independent_vars)
                
                if len(df_valid) < 10:
                    st.error(f"""
                    ‚ö†Ô∏è Dados insuficientes para an√°lise!
                    
                    - Total de registros: {len(df)}
                    - Registros v√°lidos (sem NaN): {len(df_valid)}
                    - M√≠nimo necess√°rio: 10
                    
                    Sugest√µes:
                    1. Selecione outras vari√°veis com menos valores faltantes
                    2. Selecione mais jogadores
                    3. Verifique se os dados foram carregados corretamente
                    """)
                    
                    # Mostra diagn√≥stico de valores faltantes
                    with st.expander("üìä Diagn√≥stico de Valores Faltantes"):
                        missing_stats = pd.DataFrame({
                            'Valores Faltantes': df[independent_vars + [dependent_var]].isnull().sum(),
                            'Percentual (%)': (df[independent_vars + [dependent_var]].isnull().sum() / len(df) * 100).round(2)
                        })
                        st.dataframe(missing_stats)
                else:
                    with st.spinner("Treinando o modelo de Regress√£o Linear e gerando gr√°ficos..."):
                        results = train_linear_regression(df, independent_vars, dependent_var)
                        
                        st.success("An√°lise conclu√≠da!")
                        
                        # Mostra aviso se houve imputa√ß√£o
                        if df[independent_vars].isnull().any().any():
                            st.info(f"""
                            ‚ÑπÔ∏è **Nota sobre valores faltantes:**
                            Alguns valores faltantes foram detectados e preenchidos automaticamente 
                            com a mediana das respectivas vari√°veis para permitir a an√°lise.
                            
                            - Registros originais: {len(df)}
                            - Registros ap√≥s limpeza: {len(df_valid)}
                            """)

                        # --- Se√ß√£o de Resultados ---
                        st.subheader("Resultados do Modelo")

                        # Exibe a equa√ß√£o da regress√£o
                        coef_str = " + ".join([f"({results['coefficients'].loc[var, 'Coefficient']:.4f} √ó {var})" for var in independent_vars])
                        st.markdown("**Equa√ß√£o de Regress√£o Ajustada:**")
                        st.latex(f"{dependent_var} = {results['intercept']:.4f} + {coef_str} + \\varepsilon")

                        # Exibe m√©tricas e coeficientes
                        col_metric1, col_metric2, col_metric3 = st.columns(3)
                        col_metric1.metric(label="Coeficiente de Determina√ß√£o (R¬≤)", value=f"{results['r2']:.4f}",
                                          help="Quanto da varia√ß√£o em Y √© explicada por X. Varia de 0 a 1.")
                        col_metric2.metric(label="Erro Quadr√°tico M√©dio (MSE)", value=f"{results['mse']:.4f}",
                                          help="M√©dia dos erros ao quadrado. Quanto menor, melhor.")
                        col_metric3.metric(label="Raiz do MSE (RMSE)", value=f"{np.sqrt(results['mse']):.4f}",
                                          help="Erro m√©dio em unidades da vari√°vel Y.")

                        st.write("**Coeficientes do Modelo:**")
                        st.dataframe(results['coefficients'])
                        st.info(
                            f"""
                            **Interpreta√ß√£o dos Coeficientes:** 
                            
                            Cada coeficiente (Œ≤) representa o quanto a vari√°vel dependente ({dependent_var}) 
                            muda, em m√©dia, para cada aumento de **uma unidade** na vari√°vel independente correspondente, 
                            **mantendo todas as outras vari√°veis constantes** (ceteris paribus).
                            
                            **Exemplo:** Se o coeficiente de 'FG%' for 2.5, significa que para cada aumento de 1% 
                            na porcentagem de arremessos convertidos, espera-se um aumento de 2.5 pontos em {dependent_var}.
                            """
                        )

                        # --- Se√ß√£o de Gr√°ficos ---
                        st.subheader("Visualiza√ß√µes Gr√°ficas")

                        # Gr√°fico 1: Diagrama de Dispers√£o com Linha de Regress√£o
                        st.markdown("#### 1. Diagrama de Dispers√£o com Linha de Regress√£o")
                        st.pyplot(plot_regression_scatter(
                            y_test=results['y_test'],
                            y_pred=results['y_pred'],
                            x_test_col=results['X_test'].iloc[:, 0],
                            x_label=independent_vars[0],
                            y_label=dependent_var
                        ))
                        st.caption(f"Este gr√°fico mostra a rela√ß√£o entre a vari√°vel dependente ({dependent_var}) e a primeira vari√°vel independente selecionada ({independent_vars[0]}), com a linha de regress√£o ajustada pelo modelo.")

                        # Gr√°fico 2: Previs√£o vs. Realidade
                        st.markdown("#### 2. Gr√°fico de Previs√£o vs. Realidade")
                        st.pyplot(plot_predicted_vs_actual(
                            y_test=results['y_test'],
                            y_pred=results['y_pred'],
                            y_label=dependent_var
                        ))
                        st.caption("Este gr√°fico compara os valores reais com os valores previstos pelo modelo. Pontos pr√≥ximos √† linha tracejada vermelha indicam predi√ß√µes precisas.")

                        # Gr√°fico 3: Gr√°fico de Tend√™ncia com Intervalo de Confian√ßa
                        st.markdown("#### 3. Gr√°fico de Tend√™ncia com Intervalo de Confian√ßa de 95%")
                        st.pyplot(plot_regression_confidence_interval(
                            df=df,
                            x_var=independent_vars,
                            y_var=dependent_var
                        ))
                        st.caption(f"Visualiza a tend√™ncia entre {dependent_var} e {independent_vars}. A √°rea sombreada representa o intervalo de confian√ßa de 95% para a linha de regress√£o, indicando a incerteza da estimativa.")

                        # Gr√°fico 4: Matriz de Confus√£o (Adaptada)
                        st.markdown("#### 4. Matriz de Confus√£o (Adaptada para Regress√£o)")
                        st.pyplot(plot_regression_confusion_matrix(
                            y_test=results['y_test'],
                            y_pred=results['y_pred']
                        ))
                        st.caption("Como a matriz de confus√£o √© para modelos de classifica√ß√£o, adaptamos a an√°lise: os valores foram classificados como 'Acima da M√©dia' ou 'Abaixo da M√©dia' para avaliar a capacidade do modelo de prever a magnitude do resultado.")

    # ============================================================================
    # ABA 2: EXPLORA√á√ÉO DE DADOS
    # ============================================================================
    with tabs[1]:
        st.subheader("Explora√ß√£o e An√°lise Explorat√≥ria de Dados")

        # Estat√≠sticas Descritivas
        st.markdown("#### üìä Estat√≠sticas Descritivas")
        stats_df = df[numeric_cols].describe().T
        stats_df["IQR"] = stats_df["75%"] - stats_df["25%"]
        st.dataframe(stats_df)

        # Distribui√ß√£o das Vari√°veis
        st.markdown("#### üìà Distribui√ß√£o das Vari√°veis")
        num_plots = len(numeric_cols)
        ncols = 4
        nrows = (num_plots + ncols - 1) // ncols
        
        # Fix: Adjust figure size to prevent tight_layout warnings
        fig, axes = plt.subplots(
            nrows=nrows, 
            ncols=ncols, 
            figsize=(18, 4 * nrows)  # Increased width from 16 to 18
        )
        fig.suptitle('Distribui√ß√£o das Vari√°veis', fontsize=16, fontweight='bold')

        for ax, column in zip(axes.flatten(), numeric_cols):
            sns.histplot(df[column], kde=True, ax=ax, color='skyblue', bins=30)
            ax.set_title(column, fontsize=12, fontweight='bold')
            ax.set_xlabel('')
            ax.set_ylabel('')

        # Hide unused subplots
        for ax in axes.flatten()[len(numeric_cols):]:
            ax.set_visible(False)

        plt.tight_layout()
        st.pyplot(fig)

        # Matriz de Correla√ß√£o
        st.markdown("#### üîó Correla√ß√µes entre Vari√°veis")
        correlation_matrix = df[numeric_cols].corr()
        
        fig, ax = plt.subplots(figsize=(12, 10))
        sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', linewidths=.5, ax=ax)
        ax.set_title('Mapa de Calor da Correla√ß√£o entre Vari√°veis', fontsize=14, fontweight='bold')
        st.pyplot(fig)

    # ============================================================================
    # ABA 3: AN√ÅLISE DE RES√çDUOS
    # ============================================================================
    with tabs[2]:
        st.subheader("üéÅ An√°lise de Res√≠duos - Valida√ß√£o das Premissas")
        st.markdown("""
        A an√°lise de res√≠duos valida as premissas fundamentais da regress√£o linear:
        1. **Linearidade**: Rela√ß√£o linear entre X e y
        2. **Homocedasticidade**: Vari√¢ncia constante dos erros
        3. **Normalidade**: Res√≠duos seguem distribui√ß√£o normal
        4. **Independ√™ncia**: Aus√™ncia de padr√µes nos res√≠duos
        
        **Res√≠duos**: Diferen√ßas entre valores reais e preditos: $e_i = y_i - \\hat{y}_i$
        """)

        col1, col2 = st.columns(2)
        with col1:
            dependent_var_res = st.selectbox(
                "Escolha a Vari√°vel Dependente (Y):",
                options=numeric_cols,
                index=numeric_cols.index('PTS') if 'PTS' in numeric_cols else 0,
                key="res_dependent"
            )

        available_independent_vars_res = [v for v in numeric_cols if v != dependent_var_res]

        with col2:
            independent_vars_res = st.multiselect(
                "Escolha as Vari√°veis Independentes (X):",
                options=available_independent_vars_res,
                default=[available_independent_vars_res[0]] if available_independent_vars_res else [],
                key="res_independent"
            )

        if st.button("Gerar An√°lise de Res√≠duos", type="primary", key="residuals_btn"):
            if not independent_vars_res:
                st.warning("Selecione pelo menos uma vari√°vel independente.")
            else:
                with st.spinner("Gerando an√°lise de res√≠duos..."):
                    results_res = train_linear_regression(df, independent_vars_res, dependent_var_res)
                    
                    residuals = results_res['y_test'] - results_res['y_pred']
                    standardized_residuals = (residuals - np.mean(residuals)) / np.std(residuals)

                    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
                    fig.suptitle('An√°lise de Res√≠duos - Valida√ß√£o das Premissas da Regress√£o Linear', 
                                fontsize=14, fontweight='bold', y=1.00)

                    # 1. Res√≠duos vs Valores Preditos
                    axes[0, 0].scatter(results_res['y_pred'], residuals, alpha=0.5, edgecolors='k', linewidth=0.5, s=30)
                    axes[0, 0].axhline(y=0, color='r', linestyle='--', linewidth=2, label='Res√≠duo = 0')
                    axes[0, 0].set_xlabel('Valores Preditos', fontsize=12, fontweight='bold')
                    axes[0, 0].set_ylabel('Res√≠duos', fontsize=12, fontweight='bold')
                    axes[0, 0].set_title('1. Res√≠duos vs Predi√ß√µes\n‚úì Padr√£o aleat√≥rio indica homocedasticidade', fontsize=12, fontweight='bold')
                    axes[0, 0].legend()
                    axes[0, 0].grid(True, alpha=0.3)

                    # 2. Histograma dos Res√≠duos
                    axes[0, 1].hist(residuals, bins=50, density=True, alpha=0.7, color='skyblue', edgecolor='black', label='Res√≠duos')
                    axes[0, 1].axvline(x=0, color='r', linestyle='--', linewidth=2, label='M√©dia = 0')
                    mu, sigma = residuals.mean(), residuals.std()
                    x = np.linspace(residuals.min(), residuals.max(), 100)
                    axes[0, 1].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label=f'Normal(Œº={mu:.2f}, œÉ={sigma:.2f})')
                    axes[0, 1].set_xlabel('Res√≠duos', fontsize=12, fontweight='bold')
                    axes[0, 1].set_ylabel('Densidade', fontsize=12, fontweight='bold')
                    axes[0, 1].set_title('2. Distribui√ß√£o dos Res√≠duos\n‚úì Deve seguir distribui√ß√£o normal', fontsize=12, fontweight='bold')
                    axes[0, 1].legend()
                    axes[0, 1].grid(True, alpha=0.3)

                    # 3. Q-Q Plot
                    stats.probplot(residuals, dist="norm", plot=axes[1, 0])
                    axes[1, 0].get_lines()[0].set_markerfacecolor('blue')
                    axes[1, 0].get_lines()[0].set_markeredgecolor('black')
                    axes[1, 0].get_lines()[0].set_markersize(5)
                    axes[1, 0].get_lines()[1].set_color('red')
                    axes[1, 0].get_lines()[1].set_linewidth(2)
                    axes[1, 0].set_title('3. Q-Q Plot\n‚úì Pontos na linha diagonal indicam normalidade', fontsize=12, fontweight='bold')
                    axes[1, 0].grid(True, alpha=0.3)

                    # 4. Scale-Location Plot
                    axes[1, 1].scatter(results_res['y_pred'], np.abs(standardized_residuals), alpha=0.5, edgecolors='k', linewidth=0.5, s=30)
                    axes[1, 1].axhline(y=2, color='orange', linestyle=':', linewidth=2, label='¬±2œÉ (95%)')
                    axes[1, 1].axhline(y=3, color='red', linestyle=':', linewidth=2, label='¬±3œÉ (99.7%)')
                    axes[1, 1].set_xlabel('Valores Preditos', fontsize=12, fontweight='bold')
                    axes[1, 1].set_ylabel('|Res√≠duos Padronizados|', fontsize=12, fontweight='bold')
                    axes[1, 1].set_title('4. Scale-Location Plot\n‚úì Linha horizontal indica vari√¢ncia constante', fontsize=12, fontweight='bold')
                    axes[1, 1].legend(loc='upper right')
                    axes[1, 1].grid(True, alpha=0.3)

                    plt.tight_layout()
                    st.pyplot(fig)

                    # Estat√≠sticas dos res√≠duos
                    st.markdown("#### üìä Estat√≠sticas dos Res√≠duos")
                    col1, col2, col3, col4 = st.columns(4)
                    col1.metric("M√©dia", f"{np.mean(residuals):.6f}")
                    col2.metric("Desvio Padr√£o", f"{np.std(residuals):.4f}")
                    col3.metric("M√≠nimo", f"{np.min(residuals):.4f}")
                    col4.metric("M√°ximo", f"{np.max(residuals):.4f}")

                    # Teste de Normalidade
                    st.markdown("#### üî¨ Teste de Normalidade (Shapiro-Wilk)")
                    sample_size = min(5000, len(residuals))
                    sample_residuals = np.random.choice(residuals, sample_size, replace=False)
                    statistic, p_value = stats.shapiro(sample_residuals)

                    col1, col2 = st.columns(2)
                    col1.metric("Estat√≠stica W", f"{statistic:.6f}")
                    col2.metric("p-valor", f"{p_value:.6f}")

                    if p_value > 0.05:
                        st.success("‚úì Res√≠duos s√£o normais (p > 0.05)")
                    else:
                        st.warning("‚úó H√° desvios da normalidade (p ‚â§ 0.05)")

                    # Detec√ß√£o de Outliers
                    st.markdown("#### ‚ö†Ô∏è Detec√ß√£o de Outliers")
                    outliers_2sigma = np.sum(np.abs(standardized_residuals) > 2)
                    outliers_3sigma = np.sum(np.abs(standardized_residuals) > 3)
                    total = len(residuals)

                    col1, col2, col3 = st.columns(3)
                    col1.metric("Total de observa√ß√µes", total)
                    col2.metric("Al√©m de ¬±2œÉ", f"{outliers_2sigma} ({outliers_2sigma/total*100:.2f}%)")
                    col3.metric("Al√©m de ¬±3œÉ", f"{outliers_3sigma} ({outliers_3sigma/total*100:.2f}%)")